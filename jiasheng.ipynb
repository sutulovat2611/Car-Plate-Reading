{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.util import random_noise\n",
    "from imutils import grab_contours\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02352941, 0.02352941, 0.01568627, 0.01960784, 0.01960784,\n",
       "       0.01960784, 0.02352941, 0.03529412, 0.03529412, 0.01960784,\n",
       "       0.04705882, 0.0627451 , 0.06666667, 0.07843137, 0.11372549,\n",
       "       0.15294118, 0.18823529, 0.29019608, 0.26666667, 0.28627451,\n",
       "       0.2627451 , 0.16470588, 0.04313725, 0.04705882, 0.05098039,\n",
       "       0.05490196, 0.0745098 , 0.14901961, 0.02352941, 0.01568627,\n",
       "       0.01568627, 0.01568627, 0.02352941, 0.02352941, 0.01960784,\n",
       "       0.02352941, 0.03137255, 0.05098039, 0.09411765, 0.31372549,\n",
       "       0.61960784, 0.81568627, 0.85490196, 0.85882353, 0.8627451 ,\n",
       "       0.8627451 , 0.84313725, 0.51764706, 0.18039216, 0.07058824,\n",
       "       0.09019608, 0.06666667, 0.05098039, 0.03921569, 0.03529412,\n",
       "       0.03921569, 0.01568627, 0.01568627, 0.01568627, 0.01960784,\n",
       "       0.02352941, 0.02352941, 0.04313725, 0.04313725, 0.03529412,\n",
       "       0.27843137, 0.74509804, 0.84313725, 0.80392157, 0.77254902,\n",
       "       0.78039216, 0.76862745, 0.78431373, 0.76470588, 0.77647059,\n",
       "       0.84313725, 0.85882353, 0.6       , 0.14901961, 0.07843137,\n",
       "       0.04313725, 0.03921569, 0.04313725, 0.0627451 , 0.01568627,\n",
       "       0.01960784, 0.01960784, 0.01960784, 0.02352941, 0.03529412,\n",
       "       0.04313725, 0.03529412, 0.31372549, 0.85490196, 0.81568627,\n",
       "       0.77647059, 0.76078431, 0.76470588, 0.82352941, 0.83921569,\n",
       "       0.81176471, 0.81960784, 0.76862745, 0.77647059, 0.78431373,\n",
       "       0.83529412, 0.78431373, 0.27058824, 0.0627451 , 0.04705882,\n",
       "       0.04705882, 0.05490196, 0.01568627, 0.01568627, 0.01960784,\n",
       "       0.01960784, 0.02352941, 0.02352941, 0.05490196, 0.2       ,\n",
       "       0.76078431, 0.81568627, 0.78039216, 0.76862745, 0.81568627,\n",
       "       0.81960784, 0.58823529, 0.45490196, 0.38039216, 0.54901961,\n",
       "       0.78039216, 0.83137255, 0.79215686, 0.76470588, 0.83529412,\n",
       "       0.79607843, 0.24313725, 0.05098039, 0.04705882, 0.0627451 ,\n",
       "       0.01568627, 0.01960784, 0.01960784, 0.01960784, 0.01568627,\n",
       "       0.03529412, 0.05882353, 0.52156863, 0.82745098, 0.78823529,\n",
       "       0.76862745, 0.82352941, 0.7254902 , 0.18039216, 0.08235294,\n",
       "       0.04313725, 0.04705882, 0.08235294, 0.16862745, 0.47843137,\n",
       "       0.78431373, 0.79607843, 0.77254902, 0.84705882, 0.69803922,\n",
       "       0.11764706, 0.0627451 , 0.05490196, 0.01960784, 0.01960784,\n",
       "       0.02352941, 0.01960784, 0.02745098, 0.03137255, 0.14509804,\n",
       "       0.70588235, 0.79607843, 0.78039216, 0.8       , 0.78823529,\n",
       "       0.25098039, 0.05882353, 0.03137255, 0.0627451 , 0.05490196,\n",
       "       0.0745098 , 0.05098039, 0.1372549 , 0.49019608, 0.81568627,\n",
       "       0.77254902, 0.79607843, 0.85882353, 0.35294118, 0.06666667,\n",
       "       0.05882353, 0.01960784, 0.01960784, 0.02745098, 0.03921569,\n",
       "       0.03137255, 0.03137255, 0.2       , 0.72941176, 0.80392157,\n",
       "       0.77254902, 0.81176471, 0.74117647, 0.17647059, 0.03921569,\n",
       "       0.04705882, 0.05098039, 0.04705882, 0.03921569, 0.04705882,\n",
       "       0.0627451 , 0.27843137, 0.75294118, 0.8       , 0.77254902,\n",
       "       0.83921569, 0.61176471, 0.07843137, 0.05490196, 0.02352941,\n",
       "       0.01960784, 0.02352941, 0.03921569, 0.04313725, 0.03921569,\n",
       "       0.16078431, 0.70196078, 0.80784314, 0.76862745, 0.81176471,\n",
       "       0.76078431, 0.18431373, 0.05490196, 0.03921569, 0.03921569,\n",
       "       0.03529412, 0.03137255, 0.01568627, 0.06666667, 0.17254902,\n",
       "       0.70196078, 0.80784314, 0.78431373, 0.82352941, 0.71764706,\n",
       "       0.12941176, 0.07058824, 0.01960784, 0.02352941, 0.02745098,\n",
       "       0.01960784, 0.02745098, 0.02745098, 0.11372549, 0.64705882,\n",
       "       0.83529412, 0.77647059, 0.8       , 0.78823529, 0.16470588,\n",
       "       0.05490196, 0.03137255, 0.03529412, 0.03529412, 0.04313725,\n",
       "       0.04313725, 0.03137255, 0.18823529, 0.69019608, 0.77647059,\n",
       "       0.77254902, 0.81176471, 0.72941176, 0.1372549 , 0.05098039,\n",
       "       0.01568627, 0.01960784, 0.02352941, 0.02745098, 0.03137255,\n",
       "       0.01960784, 0.07058824, 0.49411765, 0.83921569, 0.78039216,\n",
       "       0.77254902, 0.81960784, 0.38823529, 0.08627451, 0.05098039,\n",
       "       0.05098039, 0.05882353, 0.05882353, 0.04313725, 0.03529412,\n",
       "       0.27843137, 0.8       , 0.78823529, 0.76862745, 0.81176471,\n",
       "       0.74509804, 0.15686275, 0.05490196, 0.01960784, 0.01960784,\n",
       "       0.01960784, 0.02352941, 0.02352941, 0.02745098, 0.04705882,\n",
       "       0.25490196, 0.74117647, 0.81176471, 0.78823529, 0.81568627,\n",
       "       0.83137255, 0.38823529, 0.10196078, 0.07843137, 0.05098039,\n",
       "       0.05490196, 0.04313725, 0.17647059, 0.70980392, 0.81176471,\n",
       "       0.77647059, 0.76470588, 0.79215686, 0.77647059, 0.17647059,\n",
       "       0.0627451 , 0.02352941, 0.02352941, 0.01960784, 0.02352941,\n",
       "       0.02745098, 0.02352941, 0.04705882, 0.10588235, 0.41568627,\n",
       "       0.78039216, 0.82352941, 0.79215686, 0.77647059, 0.84313725,\n",
       "       0.79215686, 0.55686275, 0.34117647, 0.39215686, 0.57647059,\n",
       "       0.80392157, 0.81568627, 0.76862745, 0.76470588, 0.76862745,\n",
       "       0.79215686, 0.82352941, 0.2       , 0.05490196, 0.02352941,\n",
       "       0.01960784, 0.02352941, 0.02745098, 0.02352941, 0.03137255,\n",
       "       0.02745098, 0.04313725, 0.10196078, 0.43137255, 0.80392157,\n",
       "       0.83921569, 0.78431373, 0.77254902, 0.80392157, 0.80392157,\n",
       "       0.81960784, 0.80392157, 0.81176471, 0.78823529, 0.77254902,\n",
       "       0.77254902, 0.76862745, 0.77254902, 0.82352941, 0.83137255,\n",
       "       0.21176471, 0.07843137, 0.02352941, 0.02352941, 0.02352941,\n",
       "       0.02352941, 0.01960784, 0.02745098, 0.03529412, 0.02745098,\n",
       "       0.04313725, 0.07058824, 0.30588235, 0.67843137, 0.83921569,\n",
       "       0.81960784, 0.79607843, 0.77647059, 0.78039216, 0.77647059,\n",
       "       0.77647059, 0.8       , 0.81960784, 0.8       , 0.76470588,\n",
       "       0.76470588, 0.80392157, 0.84313725, 0.22352941, 0.08235294,\n",
       "       0.02352941, 0.02352941, 0.02352941, 0.02352941, 0.02745098,\n",
       "       0.02745098, 0.02352941, 0.03529412, 0.03529412, 0.04313725,\n",
       "       0.05490196, 0.08627451, 0.30980392, 0.59607843, 0.77647059,\n",
       "       0.83137255, 0.83921569, 0.83921569, 0.83921569, 0.83529412,\n",
       "       0.57647059, 0.48235294, 0.78823529, 0.76470588, 0.78823529,\n",
       "       0.85490196, 0.32156863, 0.0627451 , 0.01960784, 0.01960784,\n",
       "       0.01960784, 0.02352941, 0.02352941, 0.03137255, 0.02745098,\n",
       "       0.03137255, 0.04313725, 0.01960784, 0.05882353, 0.07843137,\n",
       "       0.05490196, 0.07058824, 0.12156863, 0.18431373, 0.20392157,\n",
       "       0.21176471, 0.16862745, 0.08235294, 0.09803922, 0.3372549 ,\n",
       "       0.77647059, 0.78431373, 0.77647059, 0.85882353, 0.37647059,\n",
       "       0.06666667, 0.01960784, 0.01960784, 0.01960784, 0.02352941,\n",
       "       0.02352941, 0.02745098, 0.04705882, 0.04313725, 0.04705882,\n",
       "       0.03921569, 0.08235294, 0.16078431, 0.10980392, 0.04705882,\n",
       "       0.04705882, 0.02745098, 0.04705882, 0.03529412, 0.03137255,\n",
       "       0.05490196, 0.07058824, 0.27058824, 0.77647059, 0.79607843,\n",
       "       0.78431373, 0.83921569, 0.40784314, 0.09019608, 0.01960784,\n",
       "       0.01960784, 0.01960784, 0.01960784, 0.02745098, 0.03137255,\n",
       "       0.05490196, 0.24705882, 0.78039216, 0.88627451, 0.87058824,\n",
       "       0.87058824, 0.49019608, 0.07058824, 0.05490196, 0.03529412,\n",
       "       0.03921569, 0.03529412, 0.04705882, 0.03137255, 0.04705882,\n",
       "       0.22745098, 0.76470588, 0.79215686, 0.78823529, 0.84705882,\n",
       "       0.50588235, 0.09411765, 0.01568627, 0.01568627, 0.01568627,\n",
       "       0.02352941, 0.02352941, 0.02352941, 0.07058824, 0.18431373,\n",
       "       0.75686275, 0.78823529, 0.8       , 0.82352941, 0.68235294,\n",
       "       0.10980392, 0.07058824, 0.04705882, 0.04705882, 0.03921569,\n",
       "       0.03921569, 0.05098039, 0.03921569, 0.23921569, 0.79215686,\n",
       "       0.79215686, 0.78431373, 0.83529412, 0.55686275, 0.10588235,\n",
       "       0.01960784, 0.01960784, 0.01960784, 0.02745098, 0.01960784,\n",
       "       0.01960784, 0.02745098, 0.09803922, 0.57254902, 0.81960784,\n",
       "       0.79215686, 0.78039216, 0.83921569, 0.30980392, 0.11372549,\n",
       "       0.05098039, 0.04705882, 0.03921569, 0.05490196, 0.09019608,\n",
       "       0.05882353, 0.41176471, 0.83529412, 0.78039216, 0.78823529,\n",
       "       0.85098039, 0.45098039, 0.09803922, 0.01568627, 0.01960784,\n",
       "       0.01960784, 0.01960784, 0.02352941, 0.02745098, 0.02745098,\n",
       "       0.05882353, 0.29019608, 0.8       , 0.80784314, 0.77647059,\n",
       "       0.81960784, 0.79607843, 0.26666667, 0.05098039, 0.10196078,\n",
       "       0.05098039, 0.03529412, 0.06666667, 0.25490196, 0.78039216,\n",
       "       0.81568627, 0.78039216, 0.8       , 0.81568627, 0.27843137,\n",
       "       0.0745098 , 0.01960784, 0.01960784, 0.01960784, 0.01960784,\n",
       "       0.02352941, 0.02352941, 0.02352941, 0.05490196, 0.09803922,\n",
       "       0.48235294, 0.80392157, 0.80392157, 0.78823529, 0.8       ,\n",
       "       0.8627451 , 0.78823529, 0.52941176, 0.38039216, 0.44313725,\n",
       "       0.66666667, 0.84313725, 0.82352941, 0.78431373, 0.79215686,\n",
       "       0.85882353, 0.54117647, 0.10196078, 0.07058824, 0.02352941,\n",
       "       0.01960784, 0.01568627, 0.02352941, 0.02352941, 0.02745098,\n",
       "       0.02745098, 0.03921569, 0.05490196, 0.11764706, 0.49803922,\n",
       "       0.81568627, 0.80392157, 0.78823529, 0.77647059, 0.78823529,\n",
       "       0.79607843, 0.8       , 0.80392157, 0.78823529, 0.77254902,\n",
       "       0.77254902, 0.78823529, 0.84313725, 0.68235294, 0.16078431,\n",
       "       0.04705882, 0.03137255, 0.01960784, 0.02352941, 0.01960784,\n",
       "       0.02352941, 0.02745098, 0.02745098, 0.02352941, 0.02745098,\n",
       "       0.02745098, 0.04705882, 0.12156863, 0.38431373, 0.66666667,\n",
       "       0.84313725, 0.80392157, 0.79607843, 0.78431373, 0.77647059,\n",
       "       0.78823529, 0.78431373, 0.79607843, 0.84313725, 0.85098039,\n",
       "       0.5254902 , 0.14901961, 0.05882353, 0.03137255, 0.03529412,\n",
       "       0.02745098, 0.01960784, 0.03137255, 0.01960784, 0.02745098,\n",
       "       0.02352941, 0.02745098, 0.02745098, 0.01960784, 0.03137255,\n",
       "       0.02745098, 0.03921569, 0.12156863, 0.3254902 , 0.56078431,\n",
       "       0.73333333, 0.83137255, 0.83921569, 0.83921569, 0.83137255,\n",
       "       0.74901961, 0.52156863, 0.17647059, 0.09411765, 0.05490196,\n",
       "       0.05098039, 0.04313725, 0.03529412, 0.03137255, 0.02745098,\n",
       "       0.03137255, 0.03529412, 0.01960784, 0.02352941, 0.03529412,\n",
       "       0.03137255, 0.03137255, 0.03529412, 0.03529412, 0.03529412,\n",
       "       0.02352941, 0.01960784, 0.03529412, 0.07843137, 0.10588235,\n",
       "       0.14509804, 0.12156863, 0.0745098 , 0.05882353, 0.0627451 ,\n",
       "       0.04705882, 0.05098039, 0.03529412, 0.05490196, 0.03137255,\n",
       "       0.04313725, 0.03529412, 0.01176471, 0.01568627, 0.01568627,\n",
       "       0.05098039, 0.04313725, 0.03921569, 0.03529412, 0.01960784,\n",
       "       0.01176471, 0.04313725, 0.05882353, 0.03529412, 0.03921569,\n",
       "       0.02352941, 0.02352941, 0.03137255, 0.03529412, 0.02352941,\n",
       "       0.03529412, 0.04705882, 0.04313725, 0.05490196, 0.04705882,\n",
       "       0.03529412, 0.03921569, 0.03921569, 0.02745098])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load image\n",
    "image = cv2.imread(\"character_image/9/9_01.jpg\")\n",
    "resized = cv2.resize(image, (28,28))\n",
    "# convert picture to gray scale\n",
    "img_gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "# _,img = cv2.threshold(img_gray,127,255,cv2.THRESH_BINARY)\n",
    "x_flattend = img_gray.reshape(1, 28*28)\n",
    "# plt.matshow(img_gray)\n",
    "x_flattend = np.squeeze(x_flattend)\n",
    "x_flattend = x_flattend/255\n",
    "inputs  = x_flattend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28630237, 0.79918388, 0.18867381, 0.27259284, 0.20952297,\n",
       "       0.41875261, 0.42839408, 0.56479862, 0.93119416, 0.09277087,\n",
       "       0.8041568 , 0.41337983, 0.60291477, 0.81707102, 0.10693973,\n",
       "       0.28475661])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# bias_j = np.random.uniform(0, 1, size=(HIDDEN_NEURONS,1))\n",
    "# bias_j\n",
    "# bias_k = np.random.uniform(0, 1, size=(HIDDEN_NEURONS))\n",
    "# bias_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "OUTPUT_NEURONS = 20\n",
    "INPUT_NEURONS = 28* 28\n",
    "HIDDEN_NEURONS = 16\n",
    "\n",
    "def Weight_Initialization():\n",
    "    # Initializing of the Weights. Random float number between -0.5 to 0.5 for weights.\n",
    "    np.random.seed(1)\n",
    "    inputs= np.random.uniform(-0.5, 0.5, size=(INPUT_NEURONS))\n",
    "    wji= np.random.uniform(-0.5, 0.5, size=(HIDDEN_NEURONS, INPUT_NEURONS))\n",
    "    wkj = np.random.uniform(-0.5, 0.5, size=(OUTPUT_NEURONS, HIDDEN_NEURONS))\n",
    "    bias_j = np.random.uniform(0, 1, size=(HIDDEN_NEURONS))\n",
    "    bias_k = np.random.uniform(0, 1, size=(OUTPUT_NEURONS))\n",
    "    targets = np.random.uniform(0, 1, size=(OUTPUT_NEURONS))\n",
    "    return inputs, wji,wkj,bias_j,bias_k,targets\n",
    "\n",
    "# def Read_Files():\n",
    "#     # Reading of Segmented Training Files, and Target Files.\n",
    "def Forward_Input_Hidden(inputs,wji, bias_j):\n",
    "    # Forward Propagation from Input -> Hidden Layer.\n",
    "    # Obtain the results at each neuron in the hidden layer.\n",
    "    # Calculate ğ‘ğ‘’ğ‘¡ğ‘—and ğ‘‚ğ‘¢ğ‘¡ğ‘—\n",
    "    \n",
    "    Netj = np.dot(inputs,wji.T) \n",
    "    print(\"Netj :\")\n",
    "    print(Netj)\n",
    "    Outj = 1/(1 + math.e**-(Netj + np.transpose(bias_j)))\n",
    "    print(\"Outj :\")\n",
    "    print(Outj)\n",
    "    return Netj,Outj\n",
    "\n",
    "def Forward_Hidden_Output(Netj,wkj, bias_k):\n",
    "    # Forward Propagation from Input -> Hidden Layer.\n",
    "    # Obtain the results at each neuron in the hidden layer.\n",
    "    # Calculate ğ‘ğ‘’ğ‘¡kand ğ‘‚ğ‘¢ğ‘¡k\n",
    "    Netk = np.dot(Netj,wkj.T) \n",
    "    print(\"Netk :\")\n",
    "    print(Netk)\n",
    "    Outk = 1/(1 + math.e**-(Netk + np.transpose(bias_k)))\n",
    "    print(\"Outk :\")\n",
    "    print(Outk)\n",
    "    return Netk, Outk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_for_End(Outk, targets, user_set):\n",
    "    # Check whether the total error is less than the error set by the user or the number of iterations is reached.\n",
    "    # returns true or false\n",
    "    def Error_Correction(outs, targets):\n",
    "        total_error= np.sum(((outs - targets)**2)/2)\n",
    "        return total_error\n",
    "    if Error_Correction(Outk, targets)< user_set:\n",
    "        return True\n",
    "\n",
    "    else: \n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weight_Bias_Correction_Output(Outk, targets, Outj):\n",
    "    # Correction of Weights and Bias between Hidden and Output Layer.\n",
    "    # Calculate ğ‘‘ğ‘¤ğ‘˜ğ‘˜ğ‘— and ğ‘‘ğ‘ğ‘˜ğ‘˜ğ‘—\n",
    "    dwkkj =  np.empty((0, len(Outk)))\n",
    "    for i in range(len(Outj)):\n",
    "        temp =(Outk - targets) * Outk*(1 - Outk) * Outj[i]\n",
    "        dwkkj = np.vstack([dwkkj,temp])\n",
    "    dbkkj = (Outk - targets) * Outk*(1 - Outk) \n",
    "    dwkkj = dwkkj.T\n",
    "    # print(dwkkj)\n",
    "    return dwkkj,dbkkj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weight_Bias_Correction_Hidden(outj,outk,inputs,target,wkj):\n",
    "    # Correction of Weights and Bias between Input and Hidden Layer.\n",
    "    # Calculate ğ‘‘ğ‘¤ğ‘—ğ‘—ğ‘– and ğ‘‘ğ‘ğ‘—ğ‘—ğ‘–\n",
    "    skl = (outk - target) * outk*(1-outk)\n",
    "    dwjji= np.multiply.outer(outj *(1 - outj) * np.dot(skl,wkj),inputs)\n",
    "    dbjii = outj *(1 - outj) * np.dot(skl,wkj)\n",
    "\n",
    "    return dwjji, dbjii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weight_Bias_Update(wkj,dwkkj, bias_k, dbkkj, wji, dwjji,bias_j,dbjii ):\n",
    "    # Saving_Weights_Bias() implemented inside\n",
    "    # Update Weights and Bias.\n",
    "    # Calculate ğ‘¤ğ‘˜ğ‘˜ğ‘—+ and ğ‘ğ‘˜ğ‘˜ğ‘—+\n",
    "    n = 0.5\n",
    "    wkjj = wkj - n*dwkkj\n",
    "    bkkj = bias_k - n*dbkkj\n",
    "    print(\"wk+\")\n",
    "    print(wkjj)\n",
    "    print(\"bias_k+\")\n",
    "    print(bkkj)\n",
    "\n",
    "    # Calculate ğ‘¤ğ‘—ğ‘—ğ‘–+ and ğ‘ğ‘—ğ‘—ğ‘–+\n",
    "    wjji = wji - n *dwjji\n",
    "    bjji = bias_j - n* dbjii\n",
    "    print(\"wj+\")\n",
    "    print(wjji)\n",
    "    print(\"bias_j+\")\n",
    "    print(bjji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Saving_Weights_Bias():\n",
    "#     # Save ğ‘¤ğ‘˜ğ‘˜ğ‘— and ğ‘ğ‘˜ğ‘˜ğ‘—\n",
    "#     # Save ğ‘¤ğ‘—ğ‘—ğ‘– and ğ‘ğ‘—ğ‘—ğ‘–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Netj :\n",
      "[-0.24 -0.41  0.52]\n",
      "Outj :\n",
      "[0.53991488 0.44769209 0.65021855]\n",
      "Netk :\n",
      "[0.16526677 0.05714176]\n",
      "Outk :\n",
      "[0.61426285 0.55160107]\n",
      "wk+\n",
      "[[-0.29451584  0.15454741  0.40660456]\n",
      " [ 0.38987751 -0.50839347  0.08780951]]\n",
      "bias_k+\n",
      "[0.31015745 0.1312517 ]\n",
      "wj+\n",
      "[[-0.80130992  0.19790413]\n",
      " [-0.49865269 -0.1978443 ]\n",
      " [ 0.40024883  0.40039813]]\n",
      "bias_j+\n",
      "[0.39738016 0.20269462 0.10049766]\n"
     ]
    }
   ],
   "source": [
    "wji,wkj,bias_j,bias_k,target = Weight_Initialization()\n",
    "# inputs= np.array([0.2,0.5])\n",
    "# wji= np.array([[0.1,0.2],[0.3,0.4]])\n",
    "# wkj= np.array([[0.5,0.6],[0.7,0.8]])\n",
    "# bias_j = np.array([0.2,0.2])\n",
    "# bias_k = np.array([0.4,0.4])\n",
    "# target = np.array([0.2,0.8])\n",
    "# inputs= np.array([0.5,0.8])\n",
    "# wji= np.array([[-0.8,0.2],[-0.5,-0.2],[0.4,0.4]])\n",
    "# wkj= np.array([[-0.3,0.15,0.4],[0.4,-0.5,0.1]])\n",
    "# bias_j = np.array([0.4,0.2,0.1])\n",
    "# bias_k = np.array([0.3,0.15])\n",
    "# target = np.array([0.7,0.4])\n",
    "netj,outj = Forward_Input_Hidden(inputs, wji, bias_j)\n",
    "netk,outk = Forward_Hidden_Output(outj, wkj, bias_k)\n",
    "\n",
    "dwkkj,dbkkj = Weight_Bias_Correction_Output(outk,target, outj)\n",
    "\n",
    "dwjji, dbjii = Weight_Bias_Correction_Hidden(outj,outk,inputs,target,wkj)\n",
    "\n",
    "Weight_Bias_Update(wkj,dwkkj, bias_k, dbkkj, wji, dwjji,bias_j,dbjii)\n",
    "\n",
    "# Error_Correction(outk, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
